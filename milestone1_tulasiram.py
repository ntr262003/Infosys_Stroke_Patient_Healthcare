# -*- coding: utf-8 -*-
"""Milestone1_Tulasiram.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p3uD1TEQxUYVUPUtsDu9rh2Ky-JybSMx

### **Data Exploration on Stroke Patient Healthcare data set**

**Dataset Description**

The dataset contains healthcare data related to strokes. It consists of 5110 records and 12 columns. The columns include patient demographic information, health metrics, and whether they have experienced a stroke.

**Dataset**
The dataset provided consists the list of patients and their healthcare related data:



*   id: Unique identifier for each patient
*   gender: Gender of the patient
*   age: Age of the patient
*   hypertension: 0 = No, 1 = Yes
*   heart_disease: 0 = No, 1 = Yes
*   ever_married: Marital status
*   work_type: Type of employment
*   Residence_type: Urban or Rural
*   avg_glucose_level: Average glucose level in blood
*   bmi: Body Mass Index
*   smoking_status: Smoking habits (never smoked, formerly smoked, etc.)
*   stroke: 1 if the patient had a stroke, 0 otherwise

###**(1) Defining Problem Statement and Analyzing Basic Metrics**

The goal is to develop a predictive model to determine whether a patient is at risk of having a stroke based on their demographic details, medical history, and lifestyle factors. By analyzing features such as age, hypertension, heart disease, BMI, glucose levels, and smoking status, we aim to identify patterns that indicate the likelihood of stroke occurrence. Accurate prediction can help in early intervention and medical decision-making to reduce stroke risks.

###  **(2) Import libraries and Loading the dataset**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

#To ignore the warnings and make code more representable
import warnings
warnings.filterwarnings('ignore')

#Load the healthcare.csv dataset into dataframe
url="/content/healthcare-dataset-stroke-data.csv"
df=pd.read_csv(url)

#show the top 5 records of dataset
df.head()

"""# **Data Exploration and Pre-Processing**

### **Check basic metrics and data types**

Understanding the structure of the dataset, including the number of rows and columns, and the data types of each attribute. It is a crucial step in **data exploration.**
"""

df.shape

df.info()

"""**Observations:**



*   The dataset contains 5110 rows and 12 columns
*   We can columns like "gender", "ever_married", "work_type", "residence_type" and "smoking_status" contain string values, which are represented using the "object" datatype in this dataframe
*   The columns like "age", "avg_glucose_level", "bmi" to be of float datatype
*   The columns like "id", "heart_disease", "hypertension", "stroke" to be of int datatype






"""

# Describing the statistical summary of the numerical type data
df.describe()

"""**Observations:**


*   Demographics: The average age of participants is 43 years, with a wide age range from 0.08 to 82 years, indicating a diverse population.
*   Health Conditions: Approximately 10% of participants have hypertension, while heart disease prevalence is around 5%, suggesting a relatively low incidence of these chronic conditions.
*   Stroke Incidence: Stroke occurrence is low at about 4.9%, indicating that the majority of participants have not experienced a stroke, reflecting overall good health in the population.





"""

# Statistical summary of categorical type data
df.describe(include='object')

"""*   Gender Distribution: The dataset consists of a majority of females, with 2,994 female participants compared to 2,116 males.
*   Marital Status: Most participants (approximately 65.6%) have ever been married, indicating a higher prevalence of marriage among the population.
*   Work Type: The most common work type is Private employment, with 2,925 individuals, reflecting a trend towards urban employment sectors.
*   Smoking Status: A significant portion of the population (approximately 37% or 1,892 participants) reported never smoking, indicating a potential focus on non-smoking individuals in this dataset.

### **Finding Unique Values**
"""

unique_values_example = {col: df[col].nunique() for col in df.columns}
print(unique_values_example)

"""



*   We find number of unique values from iterating from each column






"""

unique_values = {col: df[col].unique() for col in df.columns}
print(unique_values)

"""*   We find all the unique values present in each column by iterating them column by column

#### **Finding Unique Values for columns having categorical type of data**
"""

df.gender.unique()

df.ever_married.unique()

df.work_type.unique()

df.Residence_type.unique()

df.smoking_status.unique()

"""### **Check for missing values**

This is both a **data cleaning** and **data preprocessing** step. Identifying and handling missing values is considered **data cleaning** since it involves addressing the issue of incomplete data. Depending on the extent of missing data, you may need to decide how to handle it, either by imputing values or removing the affected rows/columns. Additionally, it is also a **data preprocessing** step since having missing values can impact the effectiveness of subsequent analyses, and addressing them helps ensure the data is in a suitable form for analysis.
"""

# Display the count of missing values for each column
df.isnull().sum()

# Calculate the missing values percentage for each column and to two decimal places
missing_value_percentage=(df.isnull().mean()*100).round(2)
# Display the missing values percentage for each column
print("Missing Values Percentage:\n")
print(missing_value_percentage)

"""**Observation:**
We can see only column "bmi" has around **3.93 %** of column's values are missing, while rest of the columns do **not** have any **null** values.

### **Handling null values**
"""

#Handling null values for continuous variable
df['bmi'].fillna(0,inplace=True)
df

"""
**Observation:**
* Filling missing values and replacing them with zero(0).

"""

df['bmi'].fillna(0)
np.mean(df.bmi)

df.fillna(np.mean(df.bmi))

"""**Observation:**


*   Another method is to handle missing value is by computing the **mean** of 'bmi' column and since we have averaged the value, now we replace null values with mean value computed above


"""

round(max(100*df.bmi.isnull()/len(df.bmi)),2)

"""**Observation**


*   In the given dataset 0.02 % of values are Null.
Since we have a very small percentage of null values we are dropping them for reducing time in processing them.


"""

df.dropna()